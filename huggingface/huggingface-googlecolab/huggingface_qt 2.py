# -*- coding: utf-8 -*-
"""Huggingface QT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bxyMr8PVqnB4904gGhT6WYU0w5TRYjPg
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers

from transformers import pipeline

classifier = pipeline(
    "sentiment-analysis",
    model="distilbert/distilbert-base-uncased-finetuned-sst-2-english")

#response = classifier("I like Large Language models very much") # This is a positive sentiment
#print(response)

response = classifier("I got irritate standing in line at airport") # This is a negative sentiment
print(response)

from transformers import pipeline
summerisator = pipeline("translation_en_to_hi", model="Helsinki-NLP/opus-mt-en-hi")
response = summerisator("I like Large Language models very much")
print(response )

generator = pipeline("text-generation", model="Qwen/Qwen3-0.6B")
response = generator("write the small story")

from transformers import pipeline

# Initialize the zero-shot classification pipeline with the model
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Define your input text and candidate labels
input_text = "The national team's new coach announced his strategy for the upcoming world cup."
candidate_labels = ["sports", "Politics", "Technology"]

# Perform classification
result = classifier(input_text, candidate_labels)

# Print the results
print(result)

from  transformers import AutoTokenzier

pip install transformers torch

import torch # # Using Auto Classes instead of Pipeline for sentimental Analysis
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Model name (pretrained for sentiment analysis)
MODEL_NAME = "distilbert-base-uncased-finetuned-sst-2-english"

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)

# Put model in evaluation mode
model.eval()

# Input text
text = "I really love this product, it's amazing!"

# Tokenize input
inputs = tokenizer(
    text,
    return_tensors="pt",
    truncation=True,
    padding=True
)

# Disable gradient calculation (important for inference)
with torch.no_grad():
    outputs = model(**inputs)

# Get logits
logits = outputs.logits

# Convert logits to probabilities
probabilities = torch.softmax(logits, dim=1)

# Get predicted label
predicted_class_id = torch.argmax(probabilities, dim=1).item()
label = model.config.id2label[predicted_class_id]
confidence = probabilities[0][predicted_class_id].item()

print(f"Sentiment: {label}")
print(f"Confidence: {confidence:.4f}")

from transformers import AutoTokenizer # Using Auto Classes instead of Pipeline

# Pretrained model name
MODEL_NAME = "distilbert-base-uncased"

# Load tokenizer automatically
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Input text
text = "AutoTokenizer makes NLP very easy!"

# Tokenize text
encoded = tokenizer(
    text,
    padding=True,
    truncation=True,
    return_tensors="pt"
)

print(encoded)

import torch
from transformers import AutoImageProcessor, AutoModelForImageClassification
from PIL import Image

# Load processor and model
processor = AutoImageProcessor.from_pretrained(
    "google/vit-base-patch16-224"
)
model = AutoModelForImageClassification.from_pretrained(
    "google/vit-base-patch16-224"
)

# Load image
image = Image.open("dog.jpg").convert("RGB")

# Preprocess
inputs = processor(images=image, return_tensors="pt")

# Inference
with torch.no_grad():
    outputs = model(**inputs)

# Get prediction
logits = outputs.logits
predicted_class_id = logits.argmax(-1).item()

label = model.config.id2label[predicted_class_id]

print("Predicted Dog Breed:", label)

